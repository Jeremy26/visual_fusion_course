{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Visual Fusion.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["1rbhQKptg-uK","6eZq-Put27fl"],"authorship_tag":"ABX9TyOoVkrjy9J39vY41HOwxc5G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6V2BxA_xar00"},"source":["# Welcome to Visual Fusion!\n","\n","In this course, you're going to learn how to fuse data from a LiDAR and a Camera! You will learn a lot, from 2D-3D Projections, to Point Cloud Shrinking, to Hungarian Matching...! <p>\n","\n","**We have a lot of work ahead!**\n","\n","Before we start, acknowledgement to this repo: https://github.com/kuixu/kitti_object_vis. This course has been based on this repo after seeing the great results and code! <p>\n","\n","We'll also use the **KITTI Dataset**! More on that in a second!"]},{"cell_type":"code","metadata":{"id":"tk-izanQH9iY"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","os.chdir(\"/content/drive/My Drive/Think Autonomous/SDC Course/Visual Fusion\")\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC_e1h8MICKX"},"source":["import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","from IPython.display import Image\n","from ipywidgets import interact, interactive, fixed\n","import glob\n","!pip install open3d==0.12.0 # Version 12\n","import open3d as o3d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qP3-j5OAKhT2"},"source":["image_files = sorted(glob.glob(\"data/img/*.png\"))\n","point_files = sorted(glob.glob(\"data/velodyne/*.pcd\"))\n","label_files = sorted(glob.glob(\"data/label/*.txt\"))\n","calib_files = sorted(glob.glob(\"data/calib/*.txt\"))\n","\n","print(\"There are\",len(image_files),\"images\")\n","index = 0\n","pcd_file = point_files[index]\n","image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n","cloud = o3d.io.read_point_cloud(pcd_file)\n","points= np.asarray(cloud.points)\n","\n","f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n","ax1.imshow(image)\n","ax1.set_title('Image', fontsize=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qmdm1q2mhKFm"},"source":["## Read the PCD File"]},{"cell_type":"code","metadata":{"id":"_HqRVOwxJP8R"},"source":["# Using this: https://github.com/centreborelli/pypotree\n","#!pip install pypotree \n","import pypotree \n","cloudpath = pypotree.generate_cloud_for_display(points)\n","pypotree.display_cloud_colab(cloudpath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rbhQKptg-uK"},"source":["# Optional - If your LiDAR file is in binary extension '.bin', use this piece of code to turn it into a '.pcd' and save it"]},{"cell_type":"code","metadata":{"id":"A72uTNJd-oiT"},"source":["point_files = sorted(glob.glob(\"data/velodyne/*.bin\"))\n","#index = 0 # Change if necessary"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zq7lPkvYPXow"},"source":["## BIN TO PCD\n","import numpy as np\n","import struct\n","size_float = 4\n","list_pcd = []\n","\n","file_to_open = point_files[index]\n","file_to_save = str(point_files[index])[:-3]+\"pcd\"\n","with open (file_to_open, \"rb\") as f:\n","    byte = f.read(size_float*4)\n","    while byte:\n","        x,y,z,intensity = struct.unpack(\"ffff\", byte)\n","        list_pcd.append([x, y, z])\n","        byte = f.read(size_float*4)\n","np_pcd = np.asarray(list_pcd)\n","pcd = o3d.geometry.PointCloud()\n","v3d = o3d.utility.Vector3dVector\n","pcd.points = v3d(np_pcd)\n","\n","o3d.io.write_point_cloud(file_to_save, pcd)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CwFpj39HTr4S"},"source":["# Part I - Early Fusion <p>\n","\n","In the first part of the course, we'll take a look at the early fusion process.<p>\n","Here's the process we'll follow <p>\n","\n","*   **Visualize** the Point Clouds & The Image (done)\n","*   **Project** the Point Clouds into the Image\n","*   **Detect Obstacles** in 2D\n","*   **Fuse Point Clouds & Bounding Boxes** and Get the Distance\n","\n","<p>\n","Let's go! ‚úåüèº"]},{"cell_type":"markdown","metadata":{"id":"44-hPZXmTulS"},"source":["## 1 - Project the Points onto the Image <p>\n","That part is possibly the hardest to understand and will require your full attention. We want to project the 3D points into the image.<p>\n","\n","It means we'll need to: <p>\n","\n","*   Select the Point that are **visible** in the image ü§î\n","*   Convert the Points **from the LiDAR frame to the Camera Frame** ü§Ø\n","*   Find a way to project **from the Camera Frame to the Image Frame** üò≠\n"]},{"cell_type":"code","metadata":{"id":"fSzz_3tf0xlX"},"source":["class Calibration(object):\n","    def __init__(self, calib_file):\n","        calibs = self.read_calib_file(calib_file)\n","        P = calibs[\"P2\"]\n","        self.P = np.reshape(P, [3, 4])\n","        # Rigid transform from Velodyne coord to reference camera coord\n","        V2C = calibs[\"Tr_velo_to_cam\"]\n","        self.V2C = np.reshape(V2C, [3, 4])\n","        self.C2V = self.inverse_rigid_trans(self.V2C)\n","        # Rotation from reference camera coord to rect camera coord\n","        R0 = calibs[\"R0_rect\"]\n","        self.R0 = np.reshape(R0, [3, 3])\n","    \n","    def read_calib_file(self, filepath):\n","        \"\"\" Read in a calibration file and parse into a dictionary.\n","        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n","        \"\"\"\n","        data = {}\n","        with open(filepath, \"r\") as f:\n","            for line in f.readlines():\n","                line = line.rstrip()\n","                if len(line) == 0:\n","                    continue\n","                key, value = line.split(\":\", 1)\n","                # The only non-float values in these files are dates, which\n","                # we don't care about anyway\n","                try:\n","                    data[key] = np.array([float(x) for x in value.split()])\n","                except ValueError:\n","                    pass\n","        return data\n","    \n","    def cart2hom(self, pts_3d):\n","        \"\"\" Input: nx3 points in Cartesian\n","            Oupput: nx4 points in Homogeneous by pending 1\n","        \"\"\"\n","        n = pts_3d.shape[0]\n","        pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))\n","        return pts_3d_hom\n","\n","    def inverse_rigid_trans(self,Tr):\n","        \"\"\" Inverse a rigid body transform matrix (3x4 as [R|t])\n","            [R'|-R't; 0|1]\n","        \"\"\"\n","        inv_Tr = np.zeros_like(Tr)  # 3x4\n","        inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])\n","        inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])\n","        return inv_Tr\n","    \n","    def project_rect_to_image(self,pts_3d_rect):\n","        \"\"\" Input: nx3 points in rect camera coord.\n","            Output: nx2 points in image2 coord.\n","        \"\"\"\n","        pts_3d_rect = self.cart2hom(pts_3d_rect)\n","        pts_2d = np.dot(pts_3d_rect, np.transpose(self.P))  # nx3\n","        pts_2d[:, 0] /= pts_2d[:, 2]\n","        pts_2d[:, 1] /= pts_2d[:, 2]\n","        return pts_2d[:, 0:2]\n","\n","    def project_ref_to_rect(self,pts_3d_ref):\n","        \"\"\" Input and Output are nx3 points \"\"\"\n","        return np.transpose(np.dot(self.R0, np.transpose(pts_3d_ref)))\n","\n","    def project_velo_to_ref(self,pts_3d_velo):\n","        pts_3d_velo = self.cart2hom(pts_3d_velo)  # nx4\n","        return np.dot(pts_3d_velo, np.transpose(self.V2C))\n","\n","    def project_velo_to_rect(self,pts_3d_velo):\n","        pts_3d_ref = self.project_velo_to_ref(pts_3d_velo)\n","        return self.project_ref_to_rect(pts_3d_ref)\n","\n","    def project_velo_to_image(self,pts_3d_velo):\n","        \"\"\" Input: nx3 points in velodyne coord.\n","            Output: nx2 points in image2 coord.\n","        \"\"\"\n","        pts_3d_rect = self.project_velo_to_rect(pts_3d_velo)\n","        return self.project_rect_to_image(pts_3d_rect)\n","    \n","    def get_lidar_in_image_fov(self,pc_velo, xmin, ymin, xmax, ymax, return_more=False, clip_distance=2.0):\n","        \"\"\" Filter lidar points, keep those in image FOV \"\"\"\n","        pts_2d = self.project_velo_to_image(pc_velo)\n","        fov_inds = (\n","            (pts_2d[:, 0] < xmax)\n","            & (pts_2d[:, 0] >= xmin)\n","            & (pts_2d[:, 1] < ymax)\n","            & (pts_2d[:, 1] >= ymin)\n","        )\n","        fov_inds = fov_inds & (pc_velo[:, 0] > clip_distance)\n","        imgfov_pc_velo = pc_velo[fov_inds, :]\n","        if return_more:\n","            return imgfov_pc_velo, pts_2d, fov_inds\n","        else:\n","            return imgfov_pc_velo\n","\n","    def show_lidar_on_image(self, pc_velo, img):\n","        \"\"\" Project LiDAR points to image \"\"\"\n","        img =  np.copy(img)\n","        imgfov_pc_velo, pts_2d, fov_inds = self.get_lidar_in_image_fov(\n","            pc_velo, 0, 0, img.shape[1], img.shape[0], True\n","        )\n","\n","        self.imgfov_pts_2d = pts_2d[fov_inds, :]\n","        self.imgfov_pc_rect = self.project_velo_to_rect(imgfov_pc_velo)\n","\n","        cmap = plt.cm.get_cmap(\"hsv\", 256)\n","        cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n","\n","        for i in range(self.imgfov_pts_2d.shape[0]):\n","            depth = self.imgfov_pc_rect[i, 2]\n","            color = cmap[int(640.0 / depth), :]\n","            cv2.circle(\n","                img,(int(np.round(self.imgfov_pts_2d[i, 0])), int(np.round(self.imgfov_pts_2d[i, 1]))),2,\n","                color=tuple(color),\n","                thickness=-1,\n","            )\n","        return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4osuCVzETmwu"},"source":["calib = Calibration(calib_files[index])\n","img_lidar = calib.show_lidar_on_image(points[:, :3], image)\n","img_lidar = cv2.cvtColor(img_lidar, cv2.COLOR_BGR2RGB)\n","\n","fig_lidar = plt.figure(figsize=(14, 7))\n","ax_lidar = fig_lidar.subplots()\n","ax_lidar.imshow(img_lidar)\n","plt.show()\n","cv2.imwrite(\"output/test.png\",img_lidar)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WmYX4Iex6JzH"},"source":["## 2 - Detect Obstacles in 2D"]},{"cell_type":"code","metadata":{"id":"UTEnwwS47BcJ"},"source":["!python3 -m pip install yolov4==2.0.2 # After Checking, YOLO 2.0.2 works without modifying anything. Otherwise keep 1.2.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-Py5K64gDvQ"},"source":["from yolov4.tf import YOLOv4\n","import tensorflow as tf\n","import time\n","\n","yolo = YOLOv4(tiny=False)\n","yolo.classes = \"Yolov4/coco.names\"\n","yolo.make_model()\n","yolo.load_weights(\"Yolov4/yolov4.weights\", weights_type=\"yolo\")\n","\n","def run_obstacle_detection(img):\n","    start_time=time.time()\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    resized_image = yolo.resize_image(img)\n","    # 0 ~ 255 to 0.0 ~ 1.0\n","    resized_image = resized_image / 255.\n","    #input_data == Dim(1, input_size, input_size, channels)\n","    input_data = resized_image[np.newaxis, ...].astype(np.float32)\n","\n","    candidates = yolo.model.predict(input_data)\n","\n","    _candidates = []\n","    result = img.copy()\n","    for candidate in candidates:\n","        batch_size = candidate.shape[0]\n","        grid_size = candidate.shape[1]\n","        _candidates.append(tf.reshape(candidate, shape=(1, grid_size * grid_size * 3, -1)))\n","        #candidates == Dim(batch, candidates, (bbox))\n","        candidates = np.concatenate(_candidates, axis=1)\n","        #pred_bboxes == Dim(candidates, (x, y, w, h, class_id, prob))\n","        pred_bboxes = yolo.candidates_to_pred_bboxes(candidates[0], iou_threshold=0.35, score_threshold=0.40)\n","        pred_bboxes = pred_bboxes[~(pred_bboxes==0).all(1)] #https://stackoverflow.com/questions/35673095/python-how-to-eliminate-all-the-zero-rows-from-a-matrix-in-numpy?lq=1\n","        pred_bboxes = yolo.fit_pred_bboxes_to_original(pred_bboxes, img.shape)\n","        exec_time = time.time() - start_time\n","        #print(\"time: {:.2f} ms\".format(exec_time * 1000))\n","        result = yolo.draw_bboxes(img, pred_bboxes)\n","        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n","    return result, pred_bboxes\n","\n","result, pred_bboxes = run_obstacle_detection(image)\n","fig_camera = plt.figure(figsize=(14, 7))\n","ax_lidar = fig_camera.subplots()\n","ax_lidar.imshow(result)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XJFi74VA7F0F"},"source":["## 3 - Fuse Points Clouds & Bounding Boxes"]},{"cell_type":"code","metadata":{"id":"3q6V1HE265jk"},"source":["lidar_img_with_bboxes= yolo.draw_bboxes(img_lidar, pred_bboxes)\n","fig_fusion = plt.figure(figsize=(14, 7))\n","ax_fusion = fig_fusion.subplots()\n","ax_fusion.imshow(lidar_img_with_bboxes)\n","plt.show()\n","cv2.imwrite(\"output/lidar_bboxes.png\", lidar_img_with_bboxes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0Qebe-FHViZ"},"source":["**In this course, we'll see a few ways to filter outliers.** <p>\n","Outliers are the points that belong to the bounding box, but not to the object.<p>\n","Here's an example of outliers:<p>\n","![outlier image](https://i.ibb.co/Fg0KV3k/Screenshot-2021-05-31-at-22-31-29.png)\n","\n","In this image, the points belong to the truck, but are also counted as part of the car.\n","\n","The first technique we can use for that is a shrink factor.\n","Instead of considering the whole bounding box, we're considering only a part of it. **A common choice is 10-15% shrinking.**\n","\n","![image_shrinks](https://i.ibb.co/Zcgzz6F/Screenshot-2021-05-31-at-22-45-36.png)"]},{"cell_type":"code","metadata":{"id":"k9VW-osNGbpe"},"source":["def rectContains(rect,pt, w, h, shrink_factor = 0):       \n","    x1 = int(rect[0]*w - rect[2]*w*0.5*(1-shrink_factor)) # center_x - width /2 * shrink_factor\n","    y1 = int(rect[1]*h-rect[3]*h*0.5*(1-shrink_factor)) # center_y - height /2 * shrink_factor\n","    x2 = int(rect[0]*w + rect[2]*w*0.5*(1-shrink_factor)) # center_x + width/2 * shrink_factor\n","    y2 = int(rect[1]*h+rect[3]*h*0.5*(1-shrink_factor)) # center_y + height/2 * shrink_factor\n","    return x1 < pt[0]<x2 and y1 <pt[1]<y2 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DewS97_cLjxJ"},"source":["**The second way will be through Outlier removal techniques. <p>**\n","We can cite a few: 3 Sigma, RANSAC, and others..."]},{"cell_type":"code","metadata":{"id":"bk6HfEgAsg9R"},"source":["# THREE SIGMA RULE\n","import statistics\n","\n","def filter_outliers(distances):\n","    inliers = []\n","    mu  = statistics.mean(distances)\n","    std = statistics.stdev(distances)\n","    for x in distances:\n","        if abs(x-mu) < std:\n","            # This is an INLIER\n","            inliers.append(x)\n","    return inliers\n","\n","def get_best_distance(distances, technique=\"closest\"):\n","    if technique == \"closest\":\n","        return min(distances)\n","    elif technique ==\"average\":\n","        return statistics.mean(distances)\n","    else:\n","        return statistics.median(distances)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DoTn9-yN7LUK"},"source":["distances = []\n","img_bis = image.copy()\n","\n","cmap = plt.cm.get_cmap(\"hsv\", 256)\n","cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n","\n","for box in pred_bboxes:\n","    for i in range(calib.imgfov_pts_2d.shape[0]):\n","        depth = calib.imgfov_pc_rect[i, 2]\n","        if (rectContains(box, calib.imgfov_pts_2d[i], image.shape[1], image.shape[0], shrink_factor=0.15)==True):\n","            #points_to_keep.append(calib.imgfov_pts_2d[i])\n","            distances.append(depth)\n","            color = cmap[int(640.0 / depth), :]\n","            cv2.circle(\n","            img_bis,(int(np.round(calib.imgfov_pts_2d[i, 0])), int(np.round(calib.imgfov_pts_2d[i, 1]))),2,\n","            color=tuple(color),\n","            thickness=-1,)\n","    h, w, _ = img_bis.shape\n","    distances = filter_outliers(distances)\n","    min_distance = get_best_distance(distances, technique=\"closest\")\n","    cv2.putText(img_bis, '{0:.2f} m'.format(min_distance), (int(box[0]*w),int(box[1]*h)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 3, cv2.LINE_AA)    \n","    distances_to_keep = []\n","\n","fig_keeping = plt.figure(figsize=(14, 7))\n","ax_keeping = fig_keeping.subplots()\n","ax_keeping.imshow(img_bis)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OAnmN-M6z5o_"},"source":["## Comparing with the Ground Truth\n"]},{"cell_type":"code","metadata":{"id":"r07ai06Pz7TT"},"source":["image_gt = image.copy()\n","\n","with open(label_files[index], 'r') as f:\n","    fin = f.readlines()\n","    for line in fin:\n","        if line.split(\" \")[0] != \"DontCare\":\n","            #print(line)\n","            x1_value = int(float(line.split(\" \")[4]))\n","            y1_value = int(float(line.split(\" \")[5]))\n","            x2_value = int(float(line.split(\" \")[6]))\n","            y2_value = int(float(line.split(\" \")[7]))\n","            dist = float(line.split(\" \")[13])\n","            cv2.rectangle(image_gt, (x1_value, y1_value), (x2_value, y2_value), (0,205,0), 10)\n","            cv2.putText(image_gt, str(dist), (int((x1_value+x2_value)/2),int((y1_value+y2_value)/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2, cv2.LINE_AA)    \n","\n","f, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,20))\n","ax1.imshow(image_gt)\n","ax1.set_title('Ground Truth', fontsize=30)\n","ax2.imshow(img_bis) # or flag\n","ax2.set_title('Prediction', fontsize=30)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Iu1-7LqEaDW"},"source":["## Improvements\n","\n","\n","\n","*   Use a MASK-RCNN Approach to avoid issues with Point Clouds in Bounding Boxes\n","*   Make a Faster Code optimizing for models\n","\n","\n","*   List item\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6eZq-Put27fl"},"source":["## Shooting a Portfolio Video"]},{"cell_type":"code","metadata":{"id":"VieqzsWS2-Wk"},"source":["raw_video = glob.glob(\"data/video/*.png\")\n","\n","result_video = []\n","\n","for idx,img in enumerate(raw_video):\n","    image = cv2.imread(img)\n","    result_video.append(pipeline(left_img, right_img, p_left, p_right))\n","\n","out = cv2.VideoWriter('output/out.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (w,h))\n"," \n","for i in range(len(result_video)):\n","    out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n","out.release()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1rbgM6lOBxpL"},"source":["# Part II - Late Fusion Process"]},{"cell_type":"markdown","metadata":{"id":"tXbqd3IUB3V8"},"source":["## 1 - Detect Obstacles in 2D"]},{"cell_type":"code","metadata":{"id":"OqR_PMR-Bzi3"},"source":["result, pred_bboxes = run_obstacle_detection(image)\n","fig_camera = plt.figure(figsize=(14, 7))\n","ax_lidar = fig_camera.subplots()\n","ax_lidar.imshow(result)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OpCwI9jfCESf"},"source":["## 2 - Show Obstacles in 3D LiDAR (Self-Detected)"]},{"cell_type":"code","metadata":{"id":"Ey4cpASNDNVb"},"source":["import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N-V4g0juCCRH"},"source":["cloud = o3d.io.read_point_cloud(pcd_file)\n","print(f\"Points before downsampling: {len(cloud.points)} \")\n","pcd = cloud.voxel_down_sample(voxel_size=0.2)\n","print(f\"Points after downsampling: {len(pcd.points)}\")\n","\n","print(type(pcd))\n","# RANSAC SEGMENTATION\n","plane_model, inliers = pcd.segment_plane(distance_threshold=0.25, ransac_n=3, num_iterations=100)\n","[a, b, c, d] = plane_model\n","\n","inlier_cloud = pcd.select_by_index(inliers)\n","outlier_cloud = pcd.select_by_index(inliers, invert=True)\n","outlier_cloud.paint_uniform_color([1, 0, 0])\n","\n","# CLUSTERING USING DBSCAN\n","with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Debug) as cm:\n","    clustering_labels = np.array(outlier_cloud.cluster_dbscan(eps=0.5, min_points=10, print_progress=False))\n","\n","max_label = clustering_labels.max()\n","print(f\"point cloud has {max_label + 1} clusters\")\n","\n","colors = plt.get_cmap(\"tab20\")(clustering_labels / (max_label if max_label > 0 else 1))\n","colors[clustering_labels < 0] = 0\n","outlier_cloud.colors = o3d.utility.Vector3dVector(colors[:, :3])\n","\n","o3d.visualization.draw_geometries([outlier_cloud, inlier_cloud])\n","\n","# 3D BOUNDING BOX\n","obbs = []\n","indexes = pd.Series(range(len(clustering_labels))).groupby(clustering_labels, sort=False).apply(list).tolist()\n","\n","MAX_POINTS = 300\n","MIN_POINTS = 40\n","for i in range(0, len(indexes)):\n","    nb_points = len(outlier_cloud.select_by_index(indexes[i]).points)\n","    if (nb_points>MIN_POINTS and nb_points<MAX_POINTS):\n","        sub_cloud = outlier_cloud.select_by_index(indexes[i])\n","        obb = sub_cloud.get_axis_aligned_bounding_box()\n","        obb.color = (0,0,1)\n","        obbs.append(obb)\n","\n","list_of_visuals = []\n","list_of_visuals.append(outlier_cloud)\n","list_of_visuals.extend(obbs)\n","list_of_visuals.append(inlier_cloud)\n","\n","#new_cloud = pypotree.generate_cloud_for_display(np.asarray(sub_cloud.points))\n","#pypotree.display_cloud_colab(new_cloud)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SftId832FgGq"},"source":["## 2 - Show Obstacles in 3D LiDAR (Ground Truth)"]},{"cell_type":"code","metadata":{"id":"X04lC6lcR6xx"},"source":["def compute_orientation_3d(obj, P):\n","    \"\"\" Takes an object and a projection matrix (P) and projects the 3d\n","        object orientation vector into the image plane.\n","        Returns:\n","            orientation_2d: (2,2) array in left image coord.\n","            orientation_3d: (2,3) array in in rect camera coord.\n","    \"\"\"\n","\n","    # compute rotational matrix around yaw axis\n","    R = roty(obj.ry)\n","\n","    # orientation in object coordinate system\n","    orientation_3d = np.array([[0.0, obj.l], [0, 0], [0, 0]])\n","\n","    # rotate and translate in camera coordinate system, project in image\n","    orientation_3d = np.dot(R, orientation_3d)\n","    orientation_3d[0, :] = orientation_3d[0, :] + obj.t[0]\n","    orientation_3d[1, :] = orientation_3d[1, :] + obj.t[1]\n","    orientation_3d[2, :] = orientation_3d[2, :] + obj.t[2]\n","\n","    # vector behind image plane?\n","    if np.any(orientation_3d[2, :] < 0.1):\n","        orientation_2d = None\n","        return orientation_2d, np.transpose(orientation_3d)\n","\n","    # project orientation into the image plane\n","    orientation_2d = project_to_image(np.transpose(orientation_3d), P)\n","    return orientation_2d, np.transpose(orientation_3d)\n","\n","def depth_region_pt3d(depth, obj):\n","    b = obj.box2d\n","    # depth_region = depth[b[0]:b[2],b[2]:b[3],0]\n","    pt3d = []\n","    # import pdb; pdb.set_trace()\n","    for i in range(int(b[0]), int(b[2])):\n","        for j in range(int(b[1]), int(b[3])):\n","            pt3d.append([j, i, depth[j, i]])\n","    return np.array(pt3d)\n","\n","def show_lidar_with_boxes(pc_velo, objects, calib,\n","    img_fov=False,\n","    img_width=None,\n","    img_height=None,\n","    objects_pred=None,\n","    depth=None,\n","    cam_img=None,\n","):\n","    \"\"\" Show all LiDAR points.\n","        Draw 3d box in LiDAR point cloud (in velo coord system) \"\"\"\n","\n","    # DRAW ERASED\n","    color = (0, 1, 0)\n","    for obj in objects:\n","        # Draw 3d bounding box\n","        _, box3d_pts_3d = calib.compute_box_3d(obj, calib.P)\n","        box3d_pts_3d_velo = calib.project_rect_to_velo(box3d_pts_3d)\n","\n","        # DRAW GT BOXES 3D\n","\n","        # Draw depth\n","        if depth is not None:\n","            depth_pt3d = depth_region_pt3d(depth, obj)\n","            depth_UVDepth = np.zeros_like(depth_pt3d)\n","            depth_UVDepth[:, 0] = depth_pt3d[:, 1]\n","            depth_UVDepth[:, 1] = depth_pt3d[:, 0]\n","            depth_UVDepth[:, 2] = depth_pt3d[:, 2]\n","            print(\"depth_pt3d:\", depth_UVDepth)\n","            dep_pc_velo = calib.project_image_to_velo(depth_UVDepth)\n","            print(\"dep_pc_velo:\", dep_pc_velo)\n","            # DRAW LIDAR\n","            draw_lidar(dep_pc_velo, fig=fig, pts_color=(1, 1, 1))\n","\n","        # Draw heading arrow\n","        _, ori3d_pts_3d = utils.compute_orientation_3d(obj, calib.P)\n","\n","        ori3d_pts_3d_velo = calib.project_rect_to_velo(ori3d_pts_3d)\n","        x1, y1, z1 = ori3d_pts_3d_velo[0, :]\n","        x2, y2, z2 = ori3d_pts_3d_velo[1, :]\n","        mlab.plot3d(\n","            [x1, x2],\n","            [y1, y2],\n","            [z1, z2],\n","            color=color,\n","            tube_radius=None,\n","            line_width=1,\n","            figure=fig,\n","        )\n","    if objects_pred is not None:\n","        color = (1, 0, 0)\n","        for obj in objects_pred:\n","            if obj.type == \"DontCare\":\n","                continue\n","            # Draw 3d bounding box\n","            _, box3d_pts_3d = calib.compute_box_3d(obj, calib.P)\n","            box3d_pts_3d_velo = calib.project_rect_to_velo(box3d_pts_3d)\n","            print(\"box3d_pts_3d_velo:\")\n","            print(box3d_pts_3d_velo)\n","            draw_gt_boxes3d([box3d_pts_3d_velo], fig=fig, color=color)\n","            # Draw heading arrow\n","            _, ori3d_pts_3d = utils.compute_orientation_3d(obj, calib.P)\n","            ori3d_pts_3d_velo = calib.project_rect_to_velo(ori3d_pts_3d)\n","            x1, y1, z1 = ori3d_pts_3d_velo[0, :]\n","            x2, y2, z2 = ori3d_pts_3d_velo[1, :]\n","            mlab.plot3d(\n","                [x1, x2],\n","                [y1, y2],\n","                [z1, z2],\n","                color=color,\n","                tube_radius=None,\n","                line_width=1,\n","                figure=fig,\n","            )\n","    mlab.show(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iCkYAhYjR3ZM"},"source":["## 3 - Project to the Image"]},{"cell_type":"code","metadata":{"id":"7VzAPLCxC9ti"},"source":["def read_label(label_filename):\n","    lines = [line.rstrip() for line in open(label_filename)]\n","    objects = [Object3d(line) for line in lines]\n","    return objects\n","\n","def roty(t):\n","    \"\"\" Rotation about the y-axis. \"\"\"\n","    c = np.cos(t)\n","    s = np.sin(t)\n","    return np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]])\n","\n","class Object3d(object):\n","    \"\"\" 3d object label \"\"\"\n","    def __init__(self, label_file_line):\n","        data = label_file_line.split(\" \")\n","        data[1:] = [float(x) for x in data[1:]]\n","\n","        # extract label, truncation, occlusion\n","        self.type = data[0]  # 'Car', 'Pedestrian', ...\n","        self.truncation = data[1]  # truncated pixel ratio [0..1]\n","        self.occlusion = int(data[2])  # 0=visible, 1=partly occluded, 2=fully occluded, 3=unknown\n","        self.alpha = data[3]  # object observation angle [-pi..pi]\n","\n","        # extract 2d bounding box in 0-based coordinates\n","        self.xmin = data[4]  # left\n","        self.ymin = data[5]  # top\n","        self.xmax = data[6]  # right\n","        self.ymax = data[7]  # bottom\n","        self.box2d = np.array([self.xmin, self.ymin, self.xmax, self.ymax])\n","\n","        # extract 3d bounding box information\n","        self.h = data[8]  # box height\n","        self.w = data[9]  # box width\n","        self.l = data[10]  # box length (in meters)\n","        self.t = (data[11], data[12], data[13])  # location (x,y,z) in camera coord.\n","        self.ry = data[14]  # yaw angle (around Y-axis in camera coordinates) [-pi..pi]\n","\n","class Calibration_Late(object):\n","    def __init__(self):\n","        self.P = Calibration(calib_files[index]).P\n","    \n","    def draw_projected_box3d(self,image, qs, color=(0, 255, 0), thickness=2):\n","        \"\"\" Draw 3d bounding box in image\n","            qs: (8,3) array of vertices for the 3d box in following order:\n","                1 -------- 0\n","            /|         /|\n","            2 -------- 3 .\n","            | |        | |\n","            . 5 -------- 4\n","            |/         |/\n","            6 -------- 7\n","        \"\"\"\n","        qs = qs.astype(np.int32)\n","        for k in range(0, 4):\n","            # Ref: http://docs.enthought.com/mayavi/mayavi/auto/mlab_helper_functions.html\n","            i, j = k, (k + 1) % 4\n","            # use LINE_AA for opencv3\n","            # cv2.line(image, (qs[i,0],qs[i,1]), (qs[j,0],qs[j,1]), color, thickness, cv2.CV_AA)\n","            cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n","            i, j = k + 4, (k + 1) % 4 + 4\n","            cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n","            i, j = k, k + 4\n","            cv2.line(image, (qs[i, 0], qs[i, 1]), (qs[j, 0], qs[j, 1]), color, thickness)\n","        return image\n","\n","    def project_to_image(self, pts_3d):\n","        \"\"\" Project 3d points to image plane.\n","        Usage: pts_2d = projectToImage(pts_3d, P)\n","        input: pts_3d: nx3 matrix\n","                P:      3x4 projection matrix\n","        output: pts_2d: nx2 matrix\n","        P(3x4) dot pts_3d_extended(4xn) = projected_pts_2d(3xn)\n","        => normalize projected_pts_2d(2xn)\n","        <=> pts_3d_extended(nx4) dot P'(4x3) = projected_pts_2d(nx3)\n","            => normalize projected_pts_2d(nx2)\n","        \"\"\"\n","        n = pts_3d.shape[0]\n","        pts_3d_extend = np.hstack((pts_3d, np.ones((n, 1))))\n","        # print(('pts_3d_extend shape: ', pts_3d_extend.shape))\n","        pts_2d = np.dot(pts_3d_extend, np.transpose(self.P))  # nx3\n","        pts_2d[:, 0] /= pts_2d[:, 2]\n","        pts_2d[:, 1] /= pts_2d[:, 2]\n","        return pts_2d[:, 0:2]\n","\n","    def compute_box_3d(self,obj):\n","        \"\"\" Takes an object and a projection matrix (P) and projects the 3d\n","            bounding box into the image plane.\n","            Returns:\n","                corners_2d: (8,2) array in left image coord.\n","                corners_3d: (8,3) array in in rect camera coord.\n","        \"\"\"\n","        # compute rotational matrix around yaw axis\n","        R = roty(obj.ry)\n","\n","        # 3d bounding box dimensions\n","        l = obj.l\n","        w = obj.w\n","        h = obj.h\n","\n","        # 3d bounding box corners\n","        x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n","        y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n","        z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n","\n","        # rotate and translate 3d bounding box\n","        corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n","        # print corners_3d.shape\n","        corners_3d[0, :] = corners_3d[0, :] + obj.t[0]\n","        corners_3d[1, :] = corners_3d[1, :] + obj.t[1]\n","        corners_3d[2, :] = corners_3d[2, :] + obj.t[2]\n","        # print 'cornsers_3d: ', corners_3d\n","        # only draw 3d bounding box for objs in front of the camera\n","        if np.any(corners_3d[2, :] < 0.1):\n","            corners_2d = None\n","            return corners_2d, np.transpose(corners_3d)\n","\n","        # project the 3d bounding box into the image plane\n","        corners_2d = self.project_to_image(np.transpose(corners_3d))\n","        # print 'corners_2d: ', corners_2d\n","        return corners_2d, np.transpose(corners_3d)\n","    \n","    def show_image_with_boxes(self,img, objects):\n","        \"\"\" Show image with 3D bounding boxes \"\"\"\n","        img2 = np.copy(img)  # for 3d bbox\n","        for obj in objects:\n","            box3d_pts_2d, _ = self.compute_box_3d(obj)\n","            if obj.type == \"Car\":\n","                img2 = self.draw_projected_box3d(img2, box3d_pts_2d, color=(0, 0, 255))\n","            elif obj.type == \"Pedestrian\":\n","                img2 = self.draw_projected_box3d(img2, box3d_pts_2d, color=(255, 255, 0))\n","            elif obj.type == \"Cyclist\":\n","                img2 = self.draw_projected_box3d(img2, box3d_pts_2d, color=(0, 255, 255))\n","            elif obj.type == \"Truck\":\n","                img2 = self.draw_projected_box3d(img2, box3d_pts_2d, color =(255, 0, 0))\n","        return img2\n","\n","objects = read_label(label_files[index])\n","img_bbox3d = Calibration_Late().show_image_with_boxes(image, objects)\n","\n","fig_3d = plt.figure(figsize=(14, 7))\n","ax_3d = fig_3d.subplots()\n","ax_3d.imshow(img_bbox3d)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKm1NBIjSTya"},"source":["## Shift from 3D to 2D"]},{"cell_type":"code","metadata":{"id":"HFu9OK1qJrom"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uA4Pu2cOSfuI"},"source":["## Hungarian Algorithms"]},{"cell_type":"code","metadata":{"id":"x1fJTGTgShSm"},"source":[""],"execution_count":null,"outputs":[]}]}