{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visual Fusion Early.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "1rbhQKptg-uK",
        "WmYX4Iex6JzH",
        "XJFi74VA7F0F",
        "OAnmN-M6z5o_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V2BxA_xar00"
      },
      "source": [
        "# Welcome to the Early Fusion Project\n",
        "\n",
        "Before we start, acknowledgement to this repo: https://github.com/kuixu/kitti_object_vis. This course has been based on this repo after seeing the great results and code! <p>\n",
        "\n",
        "We'll use the [KITTI Dataset](http://www.cvlibs.net/datasets/kitti/setup.php) to collect the Point Clouds, Images, and Calibration parameters. <p>\n",
        "\n",
        "After loading data from the dataset, our Late fusion process will happen in 5 steps:\n",
        "1.   **Project the Point Clouds (3D) to the Image(2D)** \n",
        "2.   **Detect Obstacles in 2D** (Camera)\n",
        "3.   **Fuse the Results**\n",
        "\n",
        "Are you ready? ‚úåüèº"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGlg7uxki1Cq"
      },
      "source": [
        "##0 - Load the Data and Visualize it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RYckDyji8H1"
      },
      "source": [
        "### Link Google Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk-izanQH9iY"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Think Autonomous/SDC Course/Visual Fusion\")\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrs7OD_Vi_Zs"
      },
      "source": [
        "### Import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pppZPZ3_jBKN"
      },
      "source": [
        "!pip install open3d==0.12.0 # Version 12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC_e1h8MICKX"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from IPython.display import Image\n",
        "from ipywidgets import interact, interactive, fixed\n",
        "import glob\n",
        "import open3d as o3d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr79FMSMpT8E"
      },
      "source": [
        "### Load the Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP3-j5OAKhT2"
      },
      "source": [
        "image_files = sorted(glob.glob(\"data/img/*.png\"))\n",
        "point_files = sorted(glob.glob(\"data/velodyne/*.pcd\"))\n",
        "label_files = sorted(glob.glob(\"data/label/*.txt\"))\n",
        "calib_files = sorted(glob.glob(\"data/calib/*.txt\"))\n",
        "\n",
        "print(\"There are\",len(image_files),\"images\")\n",
        "index = 0\n",
        "pcd_file = point_files[index]\n",
        "image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n",
        "cloud = o3d.io.read_point_cloud(pcd_file)\n",
        "points= np.asarray(cloud.points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rbhQKptg-uK"
      },
      "source": [
        "### Optional - If your LiDAR file is in binary extension '.bin', use this piece of code to turn it into a '.pcd' and save it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq7lPkvYPXow"
      },
      "source": [
        "## BIN TO PCD\n",
        "import numpy as np\n",
        "import struct\n",
        "size_float = 4\n",
        "list_pcd = []\n",
        "\n",
        "file_to_open = point_files[index]\n",
        "file_to_save = str(point_files[index])[:-3]+\"pcd\"\n",
        "with open (file_to_open, \"rb\") as f:\n",
        "    byte = f.read(size_float*4)\n",
        "    while byte:\n",
        "        x,y,z,intensity = struct.unpack(\"ffff\", byte)\n",
        "        list_pcd.append([x, y, z])\n",
        "        byte = f.read(size_float*4)\n",
        "np_pcd = np.asarray(list_pcd)\n",
        "pcd = o3d.geometry.PointCloud()\n",
        "v3d = o3d.utility.Vector3dVector\n",
        "pcd.points = v3d(np_pcd)\n",
        "\n",
        "o3d.io.write_point_cloud(file_to_save, pcd)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00YaQZjrjRpW"
      },
      "source": [
        "### Visualize the Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnSsfviYjTxb"
      },
      "source": [
        "f, (ax1) = plt.subplots(1, 1, figsize=(20,10))\n",
        "ax1.imshow(image)\n",
        "ax1.set_title('Image', fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiNl7OdgjUri"
      },
      "source": [
        "### Visualize the Point Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFRpDWNXjazV"
      },
      "source": [
        "!pip install pypotree #https://github.com/centreborelli/pypotree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HqRVOwxJP8R"
      },
      "source": [
        "import pypotree \n",
        "cloudpath = pypotree.generate_cloud_for_display(points)\n",
        "pypotree.display_cloud_colab(cloudpath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-hPZXmTulS"
      },
      "source": [
        "## 1 - Project the Points in the Image <p>\n",
        "That part is possibly the hardest to understand and will require your full attention. We want to project the 3D points into the image.<p>\n",
        "\n",
        "It means we'll need to: <p>\n",
        "\n",
        "*   Select the Point that are **visible** in the image ü§î\n",
        "*   Convert the Points **from the LiDAR frame to the Camera Frame** ü§Ø\n",
        "*   Find a way to project **from the Camera Frame to the Image Frame** üò≠\n",
        "\n",
        "<p>\n",
        "No worries here, we'll figure out everything together.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTqtmQJoBx-8"
      },
      "source": [
        "### 1.1 - Read the Calibration File\n",
        "\n",
        "The first step is to read the calibration files. For each image, we have an associated calibration file that states:<p>\n",
        "\n",
        "\n",
        "*   The instrinsic and extrinsic camera calibration parameters\n",
        "*   The velodyne to camera matrices\n",
        "*   All the other \"sensor A\" to \"sensor B\" matrices\n",
        "<p>\n",
        "They are made from this setup:<p>\n",
        "\n",
        "![link text](http://www.cvlibs.net/datasets/kitti/images/setup_top_view.png)\n",
        "\n",
        "Not everything matters to us here, only a few things:\n",
        "*    **Velo-To-Cam is a variable we'll call V2C** -- It gives the rotation and translation matrices from the Velodyne to the Left Grayscale camera.\n",
        "*    We'll also use the inverse of V2C, **C2V that means Camera to Velodyne**, that we'll compute after a rigid transform.\n",
        "*    **R0_rect is then the conversion from the Left Grayscale camera to the Left Color camera;** which is the one we're using here. This is something used in Stereo Vision to make the images co-planar.\n",
        "*   **P2 is the matrix obtained after camera calibration**. It contains the intrinsic matrix K and the extrinsic.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSzz_3tf0xlX"
      },
      "source": [
        "class LiDAR2Camera(object):\n",
        "    def __init__(self, calib_file):\n",
        "        calibs = self.read_calib_file(calib_file)\n",
        "        P = calibs[\"P2\"]\n",
        "        self.P = np.reshape(P, [3, 4])\n",
        "        # Rigid transform from Velodyne coord to reference camera coord\n",
        "        V2C = calibs[\"Tr_velo_to_cam\"]\n",
        "        self.V2C = np.reshape(V2C, [3, 4])\n",
        "        self.C2V = self.inverse_rigid_trans(self.V2C)\n",
        "        # Rotation from reference camera coord to rect camera coord\n",
        "        R0 = calibs[\"R0_rect\"]\n",
        "        self.R0 = np.reshape(R0, [3, 3])\n",
        "    \n",
        "    def inverse_rigid_trans(self,Tr):\n",
        "        \"\"\" Inverse a rigid body transform matrix (3x4 as [R|t])\n",
        "            [R'|-R't; 0|1]\n",
        "        \"\"\"\n",
        "        inv_Tr = np.zeros_like(Tr)  # 3x4\n",
        "        inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])\n",
        "        inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])\n",
        "        return inv_Tr\n",
        "\n",
        "    def read_calib_file(self, filepath):\n",
        "        \"\"\" Read in a calibration file and parse into a dictionary.\n",
        "        Ref: https://github.com/utiasSTARS/pykitti/blob/master/pykitti/utils.py\n",
        "        \"\"\"\n",
        "        data = {}\n",
        "        with open(filepath, \"r\") as f:\n",
        "            for line in f.readlines():\n",
        "                line = line.rstrip()\n",
        "                if len(line) == 0:\n",
        "                    continue\n",
        "                key, value = line.split(\":\", 1)\n",
        "                # The only non-float values in these files are dates, which\n",
        "                # we don't care about anyway\n",
        "                try:\n",
        "                    data[key] = np.array([float(x) for x in value.split()])\n",
        "                except ValueError:\n",
        "                    pass\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZWuxKSFFEDA"
      },
      "source": [
        "lidar2cam = LiDAR2Camera(calib_files[index])\n",
        "print(\"P :\"+str(lidar2cam.P))\n",
        "print(\"-\")\n",
        "print(\"RO \"+str(lidar2cam.R0))\n",
        "print(\"-\")\n",
        "print(\"Velo 2 Cam \" +str(lidar2cam.V2C))\n",
        "print(\"-\")\n",
        "print(\"Cam 2 Velo\" + str(lidar2cam.C2V))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ4qEcG9CNJq"
      },
      "source": [
        "### 1.2 - Convert into Homogeneous Coordinates\n",
        "\n",
        "The main formula we'll use will be as follows:<p>\n",
        "**Y = P2 x R0 x V2C x X** with Y being the point in pixels, and X being the point in 3D.\n",
        "For that to work, we'll first need to **convert our 3D point into homogeneous coordinates**; to then multiply it with the transposed of V2C, which is a 4x3 matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW8c4L7J7u64"
      },
      "source": [
        "def cart2hom(self, pts_3d):\n",
        "    \"\"\" Input: nx3 points in Cartesian\n",
        "        Oupput: nx4 points in Homogeneous by pending 1\n",
        "    \"\"\"\n",
        "    n = pts_3d.shape[0]\n",
        "    pts_3d_hom = np.hstack((pts_3d, np.ones((n, 1))))\n",
        "    return pts_3d_hom\n",
        "\n",
        "LiDAR2Camera.cart2hom = cart2hom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09yliUIGCSmC"
      },
      "source": [
        "### 1.3 - Build the Different Projections\n",
        "Once everything is in the correct format, we need to follow the formula linearly:\n",
        "\n",
        "0.   Start with the **3D point in Cartesian coordinates**\n",
        "1.   Convert the **3D Point into Homogeneous Coordinates**\n",
        "2.   Multiply this 3D Point with the V2C transposed to **get the point in the grasycale camera frame (Homogeneous)**\n",
        "3.   Multiply this result with R0, to **get the point in the color camera frame (homogeneous)**\n",
        "4.   **Convert again into homogeneous coordinates**, and **multiply with the camera intrinsics P transposed**\n",
        "5.   Divide by the 3rd value to **get the point in Pixel values**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcMM0Uttfjcw"
      },
      "source": [
        "print(points[:1,:3])\n",
        "\n",
        "print(np.transpose(points[:1,:3]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siG14wbNcVCK"
      },
      "source": [
        "def project_velo_to_image(self, pts_3d_velo, debug=False):\n",
        "    '''\n",
        "    Input: 3D points in Velodyne Frame [nx3]\n",
        "    Output: 2D Pixels in Image Frame [nx2]\n",
        "    '''\n",
        "    # nx3\n",
        "    homogeneous = self.cart2hom(pts_3d_velo)  # nx4\n",
        "    dotted_RT = np.dot(homogeneous, np.transpose(self.V2C)) #nx3\n",
        "    dotted_with_RO = np.transpose(np.dot(self.R0, np.transpose(dotted_RT))) #nx3\n",
        "    homogeneous_2 = self.cart2hom(dotted_with_RO) #nx4\n",
        "    pts_2d = np.dot(homogeneous_2, np.transpose(self.P))  # nx3\n",
        "\n",
        "    pts_2d[:, 0] /= pts_2d[:, 2]\n",
        "    pts_2d[:, 1] /= pts_2d[:, 2]\n",
        "    if debug==True:\n",
        "        print(\"Original 3D Point \"+str(pts_3d_velo))\n",
        "        print(\"Homogeneous \"+str(homogeneous))\n",
        "        print(\"Dotted Rt \"+str(dotted_RT))\n",
        "        print(\"Dotted RO \"+ str(dotted_with_RO))\n",
        "        print(\"3D RECT \"+ str(homogeneous_2))\n",
        "        print(\"2D PIXELS \"+ str(pts_2d))\n",
        "\n",
        "    return pts_2d[:, 0:2]\n",
        "\n",
        "LiDAR2Camera.project_velo_to_image = project_velo_to_image\n",
        "print(\"Euclidean Pixels \"+str(lidar2cam.project_velo_to_image(points[:1,:3])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0NSdCLpCgBP"
      },
      "source": [
        "### 1.4 - LiDAR in Image Field Of View"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cICtSHfD8I8Z"
      },
      "source": [
        "def get_lidar_in_image_fov(self,pc_velo, xmin, ymin, xmax, ymax, return_more=False, clip_distance=2.0):\n",
        "    \"\"\" Filter lidar points, keep those in image FOV \"\"\"\n",
        "    pts_2d = self.project_velo_to_image(pc_velo)\n",
        "    fov_inds = (\n",
        "        (pts_2d[:, 0] < xmax)\n",
        "        & (pts_2d[:, 0] >= xmin)\n",
        "        & (pts_2d[:, 1] < ymax)\n",
        "        & (pts_2d[:, 1] >= ymin)\n",
        "    )\n",
        "    fov_inds = fov_inds & (pc_velo[:, 0] > clip_distance) # We don't want things that are closer to the clip distance (2m)\n",
        "    imgfov_pc_velo = pc_velo[fov_inds, :]\n",
        "    if return_more:\n",
        "        return imgfov_pc_velo, pts_2d, fov_inds\n",
        "    else:\n",
        "        return imgfov_pc_velo\n",
        "    \n",
        "LiDAR2Camera.get_lidar_in_image_fov = get_lidar_in_image_fov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVNFq-3gCnmL"
      },
      "source": [
        "###1.5 -- Get the LiDAR Points in Pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DOf4AXe8Kqf"
      },
      "source": [
        "def show_lidar_on_image(self, pc_velo, img, debug=\"False\"):\n",
        "    \"\"\" Project LiDAR points to image \"\"\"\n",
        "    imgfov_pc_velo, pts_2d, fov_inds = self.get_lidar_in_image_fov(\n",
        "        pc_velo, 0, 0, img.shape[1], img.shape[0], True\n",
        "    )\n",
        "    if (debug==True):\n",
        "        print(\"3D PC Velo \"+ str(imgfov_pc_velo)) # The 3D point Cloud Coordinates\n",
        "        print(\"2D PIXEL: \" + str(pts_2d)) # The 2D Pixels\n",
        "        print(\"FOV : \"+str(fov_inds)) # Whether the Pixel is in the image or not\n",
        "    self.imgfov_pts_2d = pts_2d[fov_inds, :]\n",
        "    \n",
        "    #self.imgfov_pc_rect = self.project_velo_to_rect(imgfov_pc_velo)\n",
        "    \n",
        "    homogeneous = self.cart2hom(imgfov_pc_velo)\n",
        "    transposed_RT = np.dot(homogeneous, np.transpose(self.V2C))\n",
        "    dotted_RO = np.transpose(np.dot(self.R0, np.transpose(transposed_RT)))\n",
        "    self.imgfov_pc_rect = dotted_RO\n",
        "    if debug==True:\n",
        "        print(\"FOV PC Rect \"+ str(self.imgfov_pc_rect))\n",
        "\n",
        "    cmap = plt.cm.get_cmap(\"hsv\", 256)\n",
        "    cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n",
        "\n",
        "    for i in range(self.imgfov_pts_2d.shape[0]):\n",
        "        depth = self.imgfov_pc_rect[i, 2]\n",
        "        if debug==True:\n",
        "            print(\"Depth \"+str(depth))\n",
        "        color = cmap[int(510.0 / depth), :]\n",
        "        cv2.circle(\n",
        "            img,(int(np.round(self.imgfov_pts_2d[i, 0])), int(np.round(self.imgfov_pts_2d[i, 1]))),2,\n",
        "            color=tuple(color),\n",
        "            thickness=-1,\n",
        "        )\n",
        "\n",
        "    return img\n",
        "\n",
        "LiDAR2Camera.show_lidar_on_image = show_lidar_on_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaLfy002oOIG"
      },
      "source": [
        "#img_3 = lidar2cam.show_lidar_on_image(points[:,:3], image)\n",
        "img_3 = lidar2cam_video.show_lidar_on_image(point_cloud[:,:3], image)\n",
        "plt.imshow(img_3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmYX4Iex6JzH"
      },
      "source": [
        "## 2 - Detect Obstacles in 2D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTEnwwS47BcJ"
      },
      "source": [
        "!python3 -m pip install yolov4==2.0.2 # After Checking, YOLO 2.0.2 works without modifying anything. Otherwise keep 1.2.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-Py5K64gDvQ"
      },
      "source": [
        "from yolov4.tf import YOLOv4\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "yolo = YOLOv4(tiny=True)\n",
        "yolo.classes = \"Yolov4/coco.names\"\n",
        "yolo.make_model()\n",
        "yolo.load_weights(\"Yolov4/yolov4-tiny.weights\", weights_type=\"yolo\")\n",
        "\n",
        "def run_obstacle_detection(img):\n",
        "    start_time=time.time()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = yolo.resize_image(img)\n",
        "    # 0 ~ 255 to 0.0 ~ 1.0\n",
        "    resized_image = resized_image / 255.\n",
        "    #input_data == Dim(1, input_size, input_size, channels)\n",
        "    input_data = resized_image[np.newaxis, ...].astype(np.float32)\n",
        "\n",
        "    candidates = yolo.model.predict(input_data)\n",
        "\n",
        "    _candidates = []\n",
        "    result = img.copy()\n",
        "    for candidate in candidates:\n",
        "        batch_size = candidate.shape[0]\n",
        "        grid_size = candidate.shape[1]\n",
        "        _candidates.append(tf.reshape(candidate, shape=(1, grid_size * grid_size * 3, -1)))\n",
        "        #candidates == Dim(batch, candidates, (bbox))\n",
        "        candidates = np.concatenate(_candidates, axis=1)\n",
        "        #pred_bboxes == Dim(candidates, (x, y, w, h, class_id, prob))\n",
        "        pred_bboxes = yolo.candidates_to_pred_bboxes(candidates[0], iou_threshold=0.35, score_threshold=0.40)\n",
        "        pred_bboxes = pred_bboxes[~(pred_bboxes==0).all(1)] #https://stackoverflow.com/questions/35673095/python-how-to-eliminate-all-the-zero-rows-from-a-matrix-in-numpy?lq=1\n",
        "        pred_bboxes = yolo.fit_pred_bboxes_to_original(pred_bboxes, img.shape)\n",
        "        exec_time = time.time() - start_time\n",
        "        #print(\"time: {:.2f} ms\".format(exec_time * 1000))\n",
        "        result = yolo.draw_bboxes(img, pred_bboxes)\n",
        "        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
        "    return result, pred_bboxes\n",
        "\n",
        "result, pred_bboxes = run_obstacle_detection(image)\n",
        "fig_camera = plt.figure(figsize=(14, 7))\n",
        "ax_lidar = fig_camera.subplots()\n",
        "ax_lidar.imshow(result)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFi74VA7F0F"
      },
      "source": [
        "## 3 - Fuse Points Clouds & Bounding Boxes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3q6V1HE265jk"
      },
      "source": [
        "lidar_img_with_bboxes= yolo.draw_bboxes(img_3, pred_bboxes)\n",
        "fig_fusion = plt.figure(figsize=(14, 7))\n",
        "ax_fusion = fig_fusion.subplots()\n",
        "ax_fusion.imshow(lidar_img_with_bboxes)\n",
        "plt.show()\n",
        "cv2.imwrite(\"output/lidar_bboxes.png\", lidar_img_with_bboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Qebe-FHViZ"
      },
      "source": [
        "**In this course, we'll see a few ways to filter outliers.** <p>\n",
        "Outliers are the points that belong to the bounding box, but not to the object.<p>\n",
        "Here's an example of outliers:<p>\n",
        "![outlier image](https://i.ibb.co/Fg0KV3k/Screenshot-2021-05-31-at-22-31-29.png)\n",
        "\n",
        "In this image, the points belong to the truck, but are also counted as part of the car.\n",
        "\n",
        "The first technique we can use for that is a shrink factor.\n",
        "Instead of considering the whole bounding box, we're considering only a part of it. **A common choice is 10-15% shrinking.**\n",
        "\n",
        "![image_shrinks](https://i.ibb.co/Zcgzz6F/Screenshot-2021-05-31-at-22-45-36.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9VW-osNGbpe"
      },
      "source": [
        "def rectContains(rect,pt, w, h, shrink_factor = 0):       \n",
        "    x1 = int(rect[0]*w - rect[2]*w*0.5*(1-shrink_factor)) # center_x - width /2 * shrink_factor\n",
        "    y1 = int(rect[1]*h-rect[3]*h*0.5*(1-shrink_factor)) # center_y - height /2 * shrink_factor\n",
        "    x2 = int(rect[0]*w + rect[2]*w*0.5*(1-shrink_factor)) # center_x + width/2 * shrink_factor\n",
        "    y2 = int(rect[1]*h+rect[3]*h*0.5*(1-shrink_factor)) # center_y + height/2 * shrink_factor\n",
        "    return x1 < pt[0]<x2 and y1 <pt[1]<y2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DewS97_cLjxJ"
      },
      "source": [
        "**The second way will be through Outlier removal techniques. <p>**\n",
        "We can cite a few: 3 Sigma, RANSAC, and others..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk6HfEgAsg9R"
      },
      "source": [
        "import statistics\n",
        "\n",
        "def filter_outliers(distances):\n",
        "    inliers = []\n",
        "    mu  = statistics.mean(distances)\n",
        "    std = statistics.stdev(distances)\n",
        "    for x in distances:\n",
        "        if abs(x-mu) < std:\n",
        "            # This is an INLIER\n",
        "            inliers.append(x)\n",
        "    return inliers\n",
        "\n",
        "def get_best_distance(distances, technique=\"closest\"):\n",
        "    if technique == \"closest\":\n",
        "        return min(distances)\n",
        "    elif technique ==\"average\":\n",
        "        return statistics.mean(distances)\n",
        "    else:\n",
        "        return statistics.median(distances)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF8wnxMuj_zQ"
      },
      "source": [
        "def lidar_camera_fusion(self, pred_bboxes, image):\n",
        "    distances = []\n",
        "    img_bis = image.copy()\n",
        "\n",
        "    cmap = plt.cm.get_cmap(\"hsv\", 256)\n",
        "    cmap = np.array([cmap(i) for i in range(256)])[:, :3] * 255\n",
        "\n",
        "    for box in pred_bboxes:\n",
        "        for i in range(self.imgfov_pts_2d.shape[0]):\n",
        "            depth = self.imgfov_pc_rect[i, 2]\n",
        "            if (rectContains(box, self.imgfov_pts_2d[i], image.shape[1], image.shape[0], shrink_factor=0.3)==True):\n",
        "                distances.append(depth)\n",
        "                color = cmap[int(510.0 / depth), :]\n",
        "                cv2.circle(img_bis,(int(np.round(self.imgfov_pts_2d[i, 0])), int(np.round(self.imgfov_pts_2d[i, 1]))),2,color=tuple(color),thickness=-1,)\n",
        "        h, w, _ = img_bis.shape\n",
        "        distances = filter_outliers(distances)\n",
        "        min_distance = get_best_distance(distances, technique=\"average\")\n",
        "        cv2.putText(img_bis, '{0:.2f} m'.format(min_distance), (int(box[0]*w),int(box[1]*h)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 0, 0), 3, cv2.LINE_AA)    \n",
        "        distances_to_keep = []\n",
        "    \n",
        "    return img_bis, distances\n",
        "\n",
        "LiDAR2Camera.lidar_camera_fusion = lidar_camera_fusion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6w7bFcYk03p"
      },
      "source": [
        "A, B = lidar2cam.lidar_camera_fusion(pred_bboxes, result)\n",
        "\n",
        "fig_keeping = plt.figure(figsize=(14, 7))\n",
        "ax_keeping = fig_keeping.subplots()\n",
        "ax_keeping.imshow(A)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAnmN-M6z5o_"
      },
      "source": [
        "## Comparing with the Ground Truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r07ai06Pz7TT"
      },
      "source": [
        "image_gt = image.copy()\n",
        "\n",
        "with open(label_files[index], 'r') as f:\n",
        "    fin = f.readlines()\n",
        "    for line in fin:\n",
        "        if line.split(\" \")[0] != \"DontCare\":\n",
        "            #print(line)\n",
        "            x1_value = int(float(line.split(\" \")[4]))\n",
        "            y1_value = int(float(line.split(\" \")[5]))\n",
        "            x2_value = int(float(line.split(\" \")[6]))\n",
        "            y2_value = int(float(line.split(\" \")[7]))\n",
        "            dist = float(line.split(\" \")[13])\n",
        "            cv2.rectangle(image_gt, (x1_value, y1_value), (x2_value, y2_value), (0,205,0), 10)\n",
        "            cv2.putText(image_gt, str(dist), (int((x1_value+x2_value)/2),int((y1_value+y2_value)/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2, cv2.LINE_AA)    \n",
        "\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(30,20))\n",
        "ax1.imshow(image_gt)\n",
        "ax1.set_title('Ground Truth', fontsize=30)\n",
        "ax2.imshow(img_bis) # or flag\n",
        "ax2.set_title('Prediction', fontsize=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWhXl-ftiIA6"
      },
      "source": [
        "def pipeline (self, image, point_cloud):\n",
        "    \"For a pair of 2 Calibrated Images\"\n",
        "    img = image.copy()\n",
        "    # Show LidAR on Image\n",
        "    lidar_img = self.show_lidar_on_image(point_cloud[:,:3], image)\n",
        "    # Run obstacle detection in 2D\n",
        "    result, pred_bboxes = run_obstacle_detection(img)\n",
        "    # Fuse Point Clouds & Bounding Boxes\n",
        "    img_final, _ = self.lidar_camera_fusion(pred_bboxes, result)\n",
        "    return img_final\n",
        "\n",
        "LiDAR2Camera.pipeline = pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhtJIYtTGLX1"
      },
      "source": [
        "lidar2cam = LiDAR2Camera(calib_files[index])\n",
        "image = cv2.cvtColor(cv2.imread(image_files[index]), cv2.COLOR_BGR2RGB)\n",
        "cloud = o3d.io.read_point_cloud(pcd_file)\n",
        "points= np.asarray(cloud.points)\n",
        "\n",
        "plt.imshow(lidar2cam.pipeline(image, points))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eZq-Put27fl"
      },
      "source": [
        "## Shooting a Portfolio Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VieqzsWS2-Wk"
      },
      "source": [
        "video_images = sorted(glob.glob(\"data/video/images/*.png\"))\n",
        "video_points = sorted(glob.glob(\"data/video/points/*.pcd\"))\n",
        "\n",
        "# Build a LiDAR2Cam object\n",
        "lidar2cam_video = LiDAR2Camera(calib_files[0])\n",
        "\n",
        "result_video = []\n",
        "\n",
        "for idx, img in enumerate(video_images):\n",
        "    image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
        "    point_cloud = np.asarray(o3d.io.read_point_cloud(video_points[idx]).points)\n",
        "    result_video.append(lidar2cam_video.pipeline(image, point_cloud))\n",
        "\n",
        "out = cv2.VideoWriter('output/out.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, (image.shape[1],image.shape[0]))\n",
        " \n",
        "for i in range(len(result_video)):\n",
        "    out.write(cv2.cvtColor(result_video[i], cv2.COLOR_BGR2RGB))\n",
        "    #out.write(result_video[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpE17S2vjP8w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}